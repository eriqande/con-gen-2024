{
  "hash": "eff3efd02047586fa061066e60d76646",
  "result": {
    "engine": "knitr",
    "markdown": "# Probability, inference, and genotype likelihoods\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n## Preamble\n\n\nIn this session, we will spend a little time talking\nabout genotype likelihoods and \"probabilistic genotype calling.\"\n\nBut, I want to talk about these topics in the context of a little background\non Bayesian statistics and Markov chain Monte Carlo.\n\nSo, my plan for this session is to start with a simple exercise of estimating\nan allele frequency in a Bayesian manner.\n\nFirst, we'll talk about estimating the allele frequencies alone:\n\n1. Introduce acyclic directed graphs (DAGs)\n1. Talk about proportionality to the joint probability (a nice perspective)\n1. Show what inference of allele frequencies looks like if you\n   know the genotypes exactly. (We have a Shiny App for that!)\n1. Talk about how we could learn about the posterior by simulating\n   values from it.  Whether they were correlated (MCMC) or not (vanilla\n   Monte Carlo).\n\n\nAfter that we will put another layer into the model:\n\n1. Add in the idea that genotypes are not observed directly, but rather\n   underlie counts of reads of different alleles.\n1. Show that computing the posterior distribution in this case\n   is much more complicated.\n1. Provide an intuitive explanation of how MCMC works in this case\n1. Talk a little bit about what MCMC is\n1. Let everyone explore genotype likelihoods and posteriors from\nsimulated sequencing read data in another Shiny App.\n\n\nFinally, we have one last Shiny App to play with that I hope\nwill help people to understand what is meant by the phrase,\n\"propagate uncertainty about genotypes into downstream inferences.\"\n\n\n\n## Bayesian Inference\n\n### Personal Probability\n\nTo understand the Bayesian paradigm, it is useful to start with the awareness\nthat everyone has their own probabilities for events or outcomes, and these\nprobabilities are informed by each person's experience, their prior beliefs,\nthe data that they have access to, and the inferential perspectives or _models_\nthat they can bring to the task of understanding and incorporating the information that they\nhave available.\n\nWait! What kind of crazy notion of probability is this?  Many of us might be\nfamiliar with the idea of probability being interpretable as the long-run average\nfrequency of outcomes if a particular event is repeated many times.  But this\n\"personal probability\" paradigm sounds quite different...\n\nA good way to approach this is with some simple card play.  If I hand out\ncards from a shuffled deck to five people in the room and tell them not to\nlook at the cards, and then ask everyone what the probability is that the\nnext card on the deck is the ace of spades, most everyone will have the\nsame probability for that.  But if we let the 5 people look at their cards\nand then ask each, in turn, what they now think is the probability that the\ntop card is the ace of spades, they will likely update their beliefs based\non the card that they possess, as well as the responses of any cardholders\nbefore them.\n\nThere a lot of assumptions that go into these calculations.  What are they?\n\nAt any rate, it is easy to see that people have different probabilities for the\ntop card being an ace of spades depending on how much information they have.\n\n### Probability as a \"Measuring Stick\" for Uncertainty\n\nA big switch being made here is that we are now using probability as a way\nof expressing or measuring each person's \"uncertainty\" about an event.  This\nis quite a departure from thinking about probability only as a property that\ncan be discussed in connection with events that we can envision happening\nmultiple time (so that we might derive a notion of probability as the average\nfrequency of occurrence over very many trials.)\n\nThis interpretation of probability is considerably more flexible than the\n\"frequentist\" interpretation, and it is also already familiar to most of us:\nwe are already quite comfortable with being told that the probability it is\ngoing to rain today is 30%, even though there will only ever be one today. With\nprobability as a measure of uncertainty, you can use probabilities to talk about\nthings that will only happen once. What is the probability that:\n\n- There will be three more category 5 hurricanes this season?\n- Next year will be the hottest on record?\n\nNot only that, but now we can use probability to make statements about how\nuncertain we are about things that are deterministic.  For example:\n\n* What is your probability that Monrovia is the capital of Liberia?\n\nSo, if we are trying to measure:\n\n- height, then we use centimeters,\n- mass, then we use kilograms,\n- electrical resistance, we use Ohms\n- uncertainty?...we use probability...\n\n### An interesting digression\n\nProbability as a field of mathematical study did not necessarily \"grow up\" with the\nintent to be a way to measure uncertainty.  (It partly grew out of contemplation\nof \"Borel subsets\" and many much more esoteric concepts.) But the way that\nit was developed as a mathematical construct turns out to be precisely what\nis required if we desired to extend the system of logic in which 1 means\n\"absolutely true\" and 0 means \"absolutely false\" to a logical system in which\nthere could be degrees of uncertainly between 0 and 1.\n\n\n### Updating Beliefs\n\nA characteristic feature that we saw playing our card game is that when new\ninformation arrives, people update their beliefs, and hence they update\ntheir degree of undertainty (their probabilites...).\n\nClearly, if we want to be Bayesians and use probability to express uncertainty\nabout things, we need a coherent, reasonable way to update our beliefs.  This is\ndone using basic principles from probability, which were described (for inferential\npurposes) by the Reverend Thomas Bayes (born 1702), and is hence known as\nBayes' Law, or Bayes Theorem.\n\nBayes Theorem is typically expressed in an equation that probably feels\npretty opaque to most people (it certainly did to me when I was a young\nbiology graduate student.)  I remembered seeing it something like this:\nsuppose that you have observed some data $D$, and that you have $k$ different\nhypotheses for its occurrence, and for each hypothesis, $i$,  you can compute\n$P(D|H_i)$ the probability of observing D given that hypothesis $i$ is true.\n\nThe posterior probability of a certain hypothesis $H_i$ given $D$ is then:\n$$\nP(H_i|D) = \\frac{P(D|H_i)P(H_i)}{\\sum_{j = 1}^k P(D|H_j)P(H_j)}\n$$\nYack!  Let's see if we can find an easier way to talk about this!\n\nWe will (at least I hope it will be easier) and we will do so while talking\nabout a simple problem that we can build upon to make more complex models:\nestimating the frequency of an allele in a population.\n\n### A Simple Conditional Probability and DAGs\n\nAlmost all of the probabilities that we talk about routinely are\n_conditional probabilities_.  That is, they are probabilities that are\nexpressed conditional on some other variable or thing taking a certain value.\n\nHere we will start beating to death an example that involves genotype and\nallele frequencies, but bear with us, because we will end up using what we\nlearn from this example---and building upon it---throughout this\nlecture.\n\nIf we ask about the probability that an individual is homozygous\nfor the $A$ allele, we might first have to make an assumption that the population\nis in Hardy-Weinberg equilibrium, and then we also need to assess this probability\n_conditional on_ the frequency of the $A$ allele being $p_A$ in the population.\n\nSo, we could write:\n$$\nP(\\mbox{Individual is AA}~|~p_A) = p_A^2\n$$\nIf we knew the value of $p_A$, then it would be easy to do this calculation.  This is\nan example of probability \"running forward\".  We will see later that when\nwe want to do inference we need to \"run probability backward\" (which is what\nBayes' Theorem lets us do.)\n\nRecall that HWE means that the allelic type of the two gene copies carried in\nan individual are conditionally independent given the allele frequency.  Hence,\nthe probability of drawing the first gene copy as an A is $p_A$, and the probability\nthat the second gene copy is drawn as an A (given that the first was an $A$), is also $p_A$.\nSo the probability that the individual is $AA$ is the product of those: $p_A^2$.\n\nWe can draw a diagram of that called an acyclic directed graph (or DAG).  These are\nvery helpful for visualizing models as they become more complex.\n\n![](diagrams/simple1.png)\n\nThe key thing to know about DAGs is that they express the factorization of\nthe joint probability of the variables into a product of conditional\nprobabilities. (Specifically a product over nodes of the probability of\neach node conditional on its parents.)\n\nFrom the way\nthe DAG is drawn above, it is clear that we can say:\n$$\nP(\\mbox{Individual is AA}~|~p_A) = P(Y_1 = A~|~p_A) P(Y_2 = A~|~p_A) = p_A \\cdot p_A =  p_A^2\n$$\n\nHere is a quick summary of DAGs:\n\n![](slide_grabs/dag-notes.png)\n\n\n\n### A Picture of Inference of Allele Frequencies From a Sample of Genotypes\n\nThe previous discussion assumed that the allele frequency was known, and we wanted\nto compute the probability of an individual's (unknown) genotype.  For that exercise,\nwe were running from an observed node in the graph ($p_A$) to two unobserved ones.  This\nis a straightforward calculation of probabilities.\n\nMuch of the time in science, we are able to observe the variables at the end of the\narrows in a DAG, and we want to make inferences about the variables that are \"upstream\"\nof them in a DAG. For an example, let us consider the situation in which we sample $N$\ndiploids and observe their genotypes, and from that we want to infer the allele frequency\n$p_A$.\n\nThe DAG for this situation looks like this:\n\n![](diagrams/infer1.png)\n\nFrom which it is immediately apparent that the probability of all the\ndata, say $\\boldsymbol{Y} = (Y_{1,1}, Y_{1,2}, Y_{2,1}, \\ldots, Y_{N,1}, Y_{N,2})$\nis just a simple product of Bernoulli random variables:\n$$\nP(\\boldsymbol{Y}~|~p_A) = \\prod_{i=1}^N P(Y_{i,1}~|~p_A)P(Y_{i,2}~|~p_A)\n$$\nIn fact, if we define each $Y$ to be 0 or 1 as follows:\n$$\n\\begin{align}\nY = 0 & ~~~~~\\mbox{with probability} ~~~ 1 - p_A & ~~\\mbox{(i.e., it's an}~a) \\\\\nY = 1 & ~~~~~\\mbox{with probability} ~~~ p_A & ~~\\mbox{(i.e., it's an}~A)\n\\end{align}\n$$\nThen it is not too hard to see that\n$$\nP(\\boldsymbol{Y}~|~p_A) = p_A^{\\sum Y_{i,j}} (1- p_A)^{2N - \\sum Y_{i,j}}\n$$\nThis is a probability function.  But if you consider this as a function of $p_A$ with\n$\\boldsymbol{Y}$ considered as fixed, then it is often referred to as the\n_likelihood function_.\n\n\n### Methods of Inference\n\nWe see that we are trying to learn something (make inference) about\n$p_A$ (an unshaded/unobserved variable) that is upstream of our observed\ndata.  This is inference.\n\nTo put some concrete numbers on this.  Let's say that $N = 100$ diploids, and\nout of the 200 gene copies, 73 of them were $A$ alleles.\n\nThere are lots of ways that you might do inference.  Here are a few:\n\n- _Method of the Eyeball_: Look at your sample and surmise that the fraction of A alleles\nin the sample is a good estimate of the fraction of $A$ alleles in the population.\n- _Method of Moments_: This formalizes the \"Method of the Eyeball\" by equating the sample mean\nwith the population mean.\n- _Method of Maximum Likelihood_: Find the value of $p_A$ that maximizes the probability of\nobserving your sample.  In other words, consider the probability\n$P(\\boldsymbol{Y}~|~p_A)$ as a function of $p_A$, where the data,\n$\\boldsymbol{Y}$, are fixed, and then find the value of $p_A$ that maximises that _likelihood_\nfunction.\n\nAll of those methods give you a _point estimate_ for $p_A$.  An alternative to\nthese methods is to be Bayesian and find the _posterior distribution_ for $p_A$\nconditional on the data, $\\boldsymbol{Y}$.\n\nBefore we do this, we are going to want to review a view simple facts about\nmarginal, conditional, and joint probabilities.\n\n### Some important probability rules\n\nIf you have two different events $A$ (no relation, necessarily, to the big-A allele ) and $B$,\nand we use $A$ and $B$ to refer to the outcome of each, then here are some things\nthat are always true:\n\n- $P(A)$ and $P(B)$ are referred to as marginal probabilities.\n- The joint probability of $A$ and $B$ is the probability that those two outcomes\noccurred, and it can be computed as the product of a marginal probability and a conditional\nprobability, in two different ways:\n$$\nP(A,B) = P(A)P(B~|~A) = P(B)P(A~|~B)\n$$\n- This means that conditional probabilities can be computed from the joint probability:\n$$\nP(A~|~B) = \\frac{P(A,B)}{P(B)}~~~~~~~~~~~~~~~~~~~~~~~~\nP(B~|~A) = \\frac{P(A,B)}{P(A)}\n$$\n\nAnd that leads us to an expression for Bayes Theorem that I find easier to grok out:\n$$\nP(A~|~B) \\propto P(A, B)\n$$\nwhere we are thinking of $P(A|B)$ as a function of $A$ with $B$ fixed.  It is typically\neasy to compute the joint probability, $P(A,B)$, and then you just have to remember that\n$P(A~|~B)$, since it is a probability on $A$, must sum to one over all possible values of $A$.\n\nThe same is true for:\n$$\nP(B~|~A) \\propto P(A, B)\n$$\n\n### Bayesian Inference for $p_A$\n\nBack to our simple example.  If we want to do Bayesian inference for $p_A$ we\nsee that we will want to compute the _posterior probability_:\n$$\nP(p_A|\\boldsymbol{Y})\n$$\nwhich  we now know is going to be proportional to the _joint probability_:\n$$\nP(p_A,\\boldsymbol{Y})\n$$\nand what we currently have at our disposal is the _likelihood_:\n$$\nP(\\boldsymbol{Y}~|~p_A)\n$$\nWe could get the joint probability by using the likelihood in the product:\n$$\nP(p_A,\\boldsymbol{Y}) = P(\\boldsymbol{Y}~|~p_A) P(p_A)\n$$\nBut what is this $P(p_A)$?!\n\nIt is the _prior distribution_ for $p_A$.  It is a\nnecessary ingredient to be able to use the likelihood to compute the\njoint probability (and, hence, the posterior probability), and we envision\nit as a probability distribution that\nexpresses our degree of belief about $p_A$ _before we look at the data_.\n\nNote that this all boils down verbally to: **The posterior distribution is\nproportional to the prior times the likelihood**.\n\n\n### A family of priors for $p_A$\n\nSince $p_A$ is a proportion, an obvious choice for prior would\nbe a beta distribution.  The beta distribution gives a continuous prior\ndistribution on a value that is between 0 and 1.  It has two parameters,\noften called $\\alpha_1$ and $\\alpha_2$.  Here are some examples:\n\n::: {.cell}\n::: {.cell-output-display}\n![](probinf_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe beta density for a random variable $X$ has the form:\n$$\np(x | \\alpha_1, \\alpha_2) = \\frac{\\Gamma(\\alpha_1 + \\alpha_2)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)}\nx^{\\alpha_1 - 1}(1-x)^{\\alpha_2 - 1}\n$$\nThe part that looks hairy is a few Gamma functions.  Don't worry about those---it\nis a constant.  The important part (the \"kernel\", as they say...) is:\n$$\nx^{\\alpha_1 - 1}(1-x)^{\\alpha_2 - 1}\n$$\nOr, if we wanted this to a be a prior on $p_A$, the prior would be proportional to:\n$$\n{p_A}^{\\alpha_1 - 1}(1-p_A)^{\\alpha_2 - 1}\n$$\nAnd, if we wanted to be even more specific, we could choose $\\alpha_1 = \\alpha_2 = 1$ to\ngive ourselves a uniform prior which is proportional to 1:\n$$\nP(p_A) \\propto {p_A}^{1 - 1}(1-p_A)^{1 - 1} = {p_A}^{0}(1-p_A)^{0} = 1\n$$\n\n\n### A Graphical Aside\n\nJust wanting to point out that if you want to be a Bayesian, you\ncan't have any unobserved nodes on your DAG that don't have any parents.\nThose priors that you use/accept/assume are always the uppermost nodes\nin a DAG.  I usually use gray nodes to denote them. For example, here is the\nDAG for the Bayesian version of the previously shown DAG:\n\n![](diagrams/infer1-with-prior.png)\n\n\n\n### The posterior for $p_A$\n\nNow that we have a choice for our prior, $P(p_A)$, we can easily find something that\nis proportional to the posterior distribution.  What is proportional to the posterior\ndistribution? Remember, **The posterior distribution is\nproportional to the prior times the likelihood** (which, don't forget, is the\n_joint_ probability).  Remember that the likelihood looks like:\n$$\nP(\\boldsymbol{Y}~|~p_A) = p_A^{\\sum Y_{i,j}} (1- p_A)^{2N - \\sum Y_{i,j}}\n$$\nAnd, in our example, the number of $A$ alleles is 73 (out of 200), so\n$\\sum Y_{i,j} = 73$ and $2N - \\sum Y_{i,j} =  127$.\n\nSince we are only going to worry about finding something that the\nposterior probability is proportional to, we can drop any constants and\nwe get this for the prior times the likelihood:\n$$\n\\begin{align}\nP(p_A~|~\\boldsymbol{Y}) &\\propto {p_A}^{0}(1-p_A)^{0} \\times p_A^{73}(1-p_A)^{127} \\\\\n  & \\propto p_A^{73}(1-p_A)^{127} \\\\\n  & \\propto p_A^{74 - 1}(1-p_A)^{128 - 1} \\\\\n\\end{align}\n$$\nwhich is precisely the kernel of a beta distribution with parameters\n74 and 128.\n\nAha! So, the posterior here is a beta distribution with $\\alpha_1 = 74$\nand $\\alpha_2 = 128$.  We can plot that if we want:\n\n::: {.cell}\n::: {.cell-output-display}\n![](probinf_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n### Shiny Interlude #1\n\nIn fact, we can do one better than just plotting that.  I've prepared a Shiny Notebook\nthat lets you play with different values of the allele frequency ($p_T$ in this case---the\nfrequency of the $T$ base at a genome position), simulate diploid genotype data from that,\nand then compute the posterior distribution, whilst fiddling with the prior parameters,\nif desired.\n\nThe one downside of Shiny apps is that they don't run properly on the RStudio Server that has\nbeen set up for ConGen.  Nor will they run properly in the Remote Desktop version of the same.\n\nBut this is not a huge issue, since you should be able to run it on **your own** computer\nwith RStudio.  The procedure for that is\n\n1. Open up RStudio.\n2. In the R console paste this code:\n```{.r}\nif(!(\"usethis\" %in% rownames(installed.packages()))) {\n  install.packages(\"usethis\")\n}\nusethis::use_course(\"eriqande/ngs-genotype-models\")\n```\n3. You will need to answer the \"Yes\" response to a few questions.  This will download\nan RStudio project and open it.\n4. From this RStudio project's file browswer, open the file `001-allele-freq-estimation.Rmd`.\n5. If the message at the top of the file says you need some new packages, click the install option.\n6. Then Click the \"Run Document\" button.\n\n\n\n\n**If that does not work for some of the students**: you can go directly to the website:\n[https://eriqande.shinyapps.io/001-allele-freq-estimation/](https://eriqande.shinyapps.io/001-allele-freq-estimation/).  But, be warned that if too many people use that link it will overwhelm my free ShinyApps account.\n\nNow, I am going to break students into break-out groups of 4 or 5 (randomly) for about 15 minutes.\n\nTake a few minutes to introduce yourselves to one another, then play with the Shiny app and talk to one another about it as you do.  Maybe even work togethery to do these things:\n\n* Input beta distribution parameters to get these different shapes:\n    - an upward facing U\n    - a flat line\n    - a curve that keeps going up at one of the boundaries\n    - a hill with a peak at 0.3\n* Observe how the posterior distribution changes when sample size changes.\n* With a true allele frequency of $p_T = 0$, with a sample size of 50 diploids, find\nvalues of the prior parameters that will give you a posterior centered on 0.5 (in other words\na ridiculously heavy prior...)\n\nI'll try to say hello in the different break out rooms.\n\n\n\n### To the Bayesian, the posterior distribution is everything\n\nTo the Bayesian, the posterior distribution contains all the information\nthat the data has about the parameter to be estimated---in this case, $p_A$.\nAnd, the full posterior distribution, unsummarized, expresses that all that information\nas fully as can be done.\n\nHowever, sometimes you don't want to carry around a full posterior distribution,\nespecially if it is a distribution on multiple variables (multidimensional).\n\nOften you will want to summarise the posterior distribution.\n\n### Some standard ways of summarizing the posterior\n\n1. The posterior mean.  Gives a point estimate that minimizes squared error posterior loss.\n2. The posterior median.  Gives a point estimate that minimizes absolute posterior loss.\n3. The posterior mode. Gives a point estimate that minimizes 0-1 posterior loss.\n4. The Bayesian Credible Interval: provides an interval estimator. Like a confidence interval, but infinitely easier to interpret correctly...\n\n\n\n\n## Monte Carlo sampling from a posterior distribution\n\nIn the above segment, we obtained a posterior distribution analytically.  That\nis not the way things typically work out.  It is more common that\none is not able to obtain the posterior analytically, but, rather, can only\nobtain a _sample_ from the posterior distribution.  With a sample from the posterior,\nhowever, it is possible to use Monte Carlo to approximate virtually any quantity\nthat might be of interest.  Let's look at that with our simple example.\n\n### A \"vanilla\" Monte Carlo sample from the posterior for $p_A$\n\nEven though we can compute the posterior analytically, it can still\nbe convenient to obtain a sample from it.  We can sample 1 million values\nfrom the posterior for $p_A$ like this:\n\n::: {.cell}\n\n```{.r .cell-code}\npA <- rbeta(n = 10^6, 74, 128)\n```\n:::\n\nThe first 10 values of that sample look like:\n\n::: {.cell}\n\n```{.r .cell-code}\npA[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.2975726 0.3143922 0.3714106 0.3811800 0.3362782 0.3539191 0.3795900\n [8] 0.3635794 0.3647453 0.2849166\n```\n\n\n:::\n:::\n\n\nWe call this a \"vanilla\" Monte Carlo sample because every member of the sample\nwas independent---this is not _Markov chain_ Monte Carlo.\n\nIf we wanted to use the sample to approximate the full posterior, we could do\nthat with a histogram:\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(pA, breaks = 100)\n```\n\n::: {.cell-output-display}\n![](probinf_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nIf we want the posterior mean, that is easy, too:\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(pA)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3663628\n```\n\n\n:::\n:::\n\nas is the posterior median:\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian(pA)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3659352\n```\n\n\n:::\n:::\n\nOr the standard deviation of the posterior distribution:\n\n::: {.cell}\n\n```{.r .cell-code}\nsd(pA)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03382754\n```\n\n\n:::\n:::\n\nOr, the 90%-equal-tail Credible Interval\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(pA, probs = c(0.05, 0.95))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       5%       95% \n0.3114930 0.4227952 \n```\n\n\n:::\n:::\n\nAll of those quantities could have been obtained analytically in this case,\nbut it is a lot simpler to just work with a Monte Carlo sample because the\noperations are the same with every Monte Carlo sample (which is not necessarily\ntrue of every analytical distribution...).\n\n### Transformations are simple with a Monte Carlo sample\n\nImagine for a moment that what you really wanted to estimate was the\nprobability that, if you sampled three individuals, in sequence, from the population,\nthe first one would be homozygous $AA$, the second one would be heterozygous $Aa$\nand the third one would be homozygous $aa$. Since we are being Bayesians here,\nwe will want to compute the posterior distribution of that probability given the\nobserved data.  This is something that would be quite messy to do analytically,\nbut it is easy to use the Monte Carlo sample to do it.  Clearly, for a given $p_A$,\nthe probability of the sequence ($AA$, $Aa$, $aa$) is\n$p_A^2\\times 2 p_A (1-p_A) \\times (1-p)^2$.  So, the posterior distribution of that\nquantity is simply:\n\n::: {.cell}\n\n```{.r .cell-code}\nthree_geno_seq_posterior <- pA^2 * 2 * pA * (1 - pA) * (1 - pA)^2\n```\n:::\n\nAnd we can look at it:\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(three_geno_seq_posterior, breaks = 100)\n```\n\n::: {.cell-output-display}\n![](probinf_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Probabilistic Genotype Calling / Allele frequency estimation {#probcall}\n\nWe are now going to expand our allele frequency model just a little bit, to get a taste for\nhow we might infer both allele frequencies and genotypes given short read data.\n\nLet's consider this setup: we are trying to infer genotypes in individuals and estimate\nthe allele frequencies in a population at a SNP in which the variants are the two\ndifferent bases,  $C$ or $T$.  We know where the SNP occurs (we are disregarding the problem\nof inferring whether a SNP is there) but we do not know its frequency in the population.\nWe have sequenced $N$ individuals at this SNP.  We don't know the genotype of any of these\nindividuals, but we get to observe the number of sequencing reads on each individual, $i$,\nthat contain a $C$, and the number of reads that contain a $T$ at the site.  We call that\nobserved variable $\\boldsymbol{R}_i = (R_{i,C}, R_{i,T})$, a two-vector.  We include in our model\na sequencing error rate $\\mu$ which, for simplicity here, we will assume is known.\nWith probability $\\mu$ a read from a chromosome that really contains a $C$ at the site will\nreport a $T$, while with probability $1 - \\mu$ the base at the site is correctly reported as a $C$.  Similarly for chromosomes that contain a $T$ at the site, with probability $\\mu$ a read\nwill report a $C$ and with probability $1 - \\mu$ it will correctly report a $T$.\n\nHere is a DAG for the model:\n\n![](diagrams/genos-and-reads-dag.png)\n\n\n\nAll right.  This is fun.  Now there are _two_ layers of unknown variables (the $Y$'s and $p_C$\nthat we will try to infer together, at the same time).\n\n### Why is this a good idea?\n\nBecause the allele frequency estimate is there to inform the inference of genotypes.\n\nIf the frequency of the $C$ allele is quite low, that will provide a lot of\nevidence in the model against inferring someone to be homozygous for the $C$ allele.\n\nAnd this would be a good thing, because after a little bit of looking at published\ndata sets of genotypes called by traditional means (i.e. apparently without\nconsidering allele frequencies), it appears there is a bit of an epidemic\nof genotyping error in RAD sequenced data.\n\n\n### Conditional probabilities in the model\n\nMost of these we have seen before:\n\n- $P(p_C | \\alpha_C, \\alpha_T)$ is just a beta prior on $p_C$.\n- $Y_{i,1}$ is a Bernoulli trial with success probability (i.e. probability of\ngetting a $C$ of $p_C$.)\n- The only new one is the probability distribution of read counts given the genotype and\nthe genotyping error rate: $P(\\boldsymbol{R}_i~|~Y_{i,1}, Y_{i,2}, \\mu)$\n\n### The conditional probability $P(\\boldsymbol{R}_i~|~Y_{i,1}, Y_{i,2}, \\mu)$\n\nThis thing turns out to be pretty easy.  It can be broken down into cases:\n\n1. **$i$ is homozygous for $C$ (i.e., $Y_{i,1} = Y_{i,2} = C$):** In this case,\nthe only way to get a read with a $T$ at the site is by a sequencing error, which\nhappens with probability $\\mu$.  Thus, it is akin to drawing $M = R_{i,C} + R_{i,T}$\nballs out of an urn (with replacement), a fraction $\\mu$ of which\nsay $T$ on them, the rest which say\n$C$ on them.  In such a case $R_{i,C}$ is binomially distributed, $M$ draws with\nsuccess probability $1-\\mu$.\n2. **$i$ is homozygous for $T$ (i.e., $Y_{i,1} = Y_{i,2} = T$):** Same argument\nas above, $R_{i,C}$ is binomially distributed, $M$ draws with\nsuccess probability $\\mu$ this time, rather than $\\mu$.\n3. **$i$ is heterozygous (i.e., $(Y_{i,1} = C, Y_{i,2} = T)$\nor $(Y_{i,1} = T, Y_{i,2} = C)$):**  Because we have assumed sequencing error rates\nare symmetrical between different bases, this is easily figured out---the reads are\nlike balls you are sampling (with replacement) from an Urn in which half of the balls\nare $T$'s and half of them are $C$'s.\n\nNote that when this probability is considered a function of the genotype, (with the\nobserved reads treated as fixed), it is know as the _genotype likelihood_:\n$$\nL(Y_{i,1}, Y_{i,2}; \\boldsymbol{R}_i, \\mu) = P(\\boldsymbol{R}_i~|~Y_{i,1}, Y_{i,2}, \\mu)\n$$\n\nAn easier way to see this might be a picture:\n\n![](figures/genotypes_and_reads.svg)\n\n_Genotype likelihoods_ are measures of how much support the _read_ data gives\nto the three possible genotypes ($CC$, $CT$, or $TT$, in this case).\n\nThey are a critical ingredient for using read information for doing\npopulation genomics, but they are only part of the story.\n\n### Some of the assumptions in the above model...\n\nThere are quite a few but the biggest is that each read is independent of the others.\nThat assumption would be violated if:\n\n- the sequences upon which the $C$ (or the $T$) occurred were somehow subject to\ngreater amplification during any PCR steps, or were more likely to attach to\n\"the lawn\" on an Illumina machine.\n- there are PCR duplicates.  This can be a really big issue for RAD methods that\ndon't have a good mechanism for filtering PCR duplicates.\n\n\n\n### An intuitive picture of how Monte Carlo might proceed\n\nThe unobserved genotypes of the individuals in the above model are what are sometimes\ncalled \"missing data\".  They are not missing data in the sense of small holes in your\ndata set that resulted from some data-collection failure.  They are missing in the sense\nof \"What is missing here is something that would make this whole problem easier\".  Clearly,\nas we showed in an earlier section, if we knew these genotypes exactly, doing Bayesian\ninference for $p_C$ is pretty darn easy.\n\nAnother name for this \"missing data\" is \"latent variables.\"  Many formulations of MCMC\nare developed with these types of latent variables.  In our case, a brief sketch of how\nMCMC would proceed starts with a bit of wishful thinking.  We say, \"Wouldn't it be great if\nwe actually knew the value of $p_C$...Hey! We don't, but let's just start with a guess and\npretend it is true.\"  So, we would start by initializing the model and setting $p_C$ to some\nrandom value, let us call that value $p_C^{(0)}$.\n\nNow, if we know the value of $p_C$, then the only thing that would be left unknown in the model\nwould be the genotypes of the individuals.  But looking at the graph it is clear that they\nare all conditionally indendent given $p_C^{(0)}$ and the observed read depths.  So,\npretending that $p_C^{(0)}$ is the truth, we could compute, for each individual $i$, the\nposterior probabilities of $i$'s genotype (the three cases, 1--3 above). (By the way, this\ndistribution is often called the \"full conditional distribution\").\n\nThen we would simulate values of the genotypes from those posterior distributions.  Aha!\nonce we have those simulated, we can pretend that they are real and use them to\ncome up with a new estimate of $p_C$ by computing the posterior distribution (full conditional)\nfor $p_C$ given the current values of the genotypes and then simulating a value from\nthat distribution.  Call that $p_C^{(1)}$, and then go back to simulating new values for the\ngenotypes.\n\nDoing that over and over again creates a Markov chain that does a random walk\nover the space of all possible genotypes of the $i$ individuals and the unknown\nallele frequency $p_C$, with the amount of time the chain spends in any state being\n_proportional to the posterior probability of that state!_.\n\nThis is actually Markov chain Monte Carlo.\n\n\n### What is the Markov Chain part of MCMC?\n\nWe've seen that Monte Carlo in this Bayesian context is all about basing\none's inference on a _sample_ from the posterior distribution.  However, it turns\nout that you often can't obtain _independent_ samples from a complex posterior distribution.\nIn high dimensions\nthat is just not possible.  But, it _is_ possible to construct an ergodic Markov chain that\nhas as its limiting distribution the posterior distribution you want to be sampling from. In\nfact, that is what our \"wishful thinking\" procedure with the \"missing data\", described\nabove, is actually doing.\n\nWhat?!\n\nOK, we will talk briefly about Markov chains.  Basically a Markov chain is a\nstochastic process (like a random walk) in which the probability of ending up\nin some place after the next step depends only on where it currently is---not\non how it got there...\n\nA classic example is a random walk, we will consider one with scattering boundaries:\n\n![](slide_grabs/random-walk.png)\n\nThe important take-home message from the `tpm` Computer Demo is that if you run a Markov chain\nlong enough, it spends time in different states with a predictable pattern called the\nlimiting distribution.  In other words, you can take a sample of the states visited by the\nchain and use that as a Monte Carlo sample.  That is Markov chain Monte Carlo.\n\n\n### Why MCMC is so great for Bayesians\n\nVia MCMC, it is possible to obtain samples from the posterior distribution\neven if the posterior distribution is highly-multidimensional and complicated.\n\nThere are two main reasons for this:\n\n1. A Markov chain can be devised to sample from the posterior distribution\nwithout knowing the normalizing constant.  In other words, if you can compute the\njoint probability (prior times likelihood) for a fully-observed version of your model,\nthat is all you need. (Note that you can pretty much always compute the joint probability\nfor a fully-observed model.)\n2. The Markov chain so devised can be constructed by proposing changes only to small\nparts of the model, which makes it possible to break down complex distributions in\nways that make it manageable to sample from them.\n\nSadly, the details of this are beyond the scope of a 40 minute lecture and a\nfew hands-on sessions.\n\n### Why MCMC can be not-so-great\n\n1. It can be computationally expensive and take a long time.\n2. It can mix poorly, i.e. it might not sample reliably from all areas\nof the distribution in proportion to (or even approximately in proportion to) their\nposterior probability.\n3. It can appear to be mixing well, but actually not be mixing well.  It is important to\nperform multiple runs from different starting values to assess convergence.\n4. Especially in \"canned packages\" it can let you do \"black-boxy\" inference in models that are\nso complicated that it is hard to have proper intuition about how they behave or perform given\nthe vararies of your own data.\n\nAgain, these topics are beyond the scope of this presentation.  But grab me at some point if you want to talk about it, or consider the MCMC course at the [Summer Institute in Statistical Genetics](https://si.biostat.washington.edu/suminst/sisg).\n\n\n### Playing with genotype likelihoods and genotype posteriors\n\nThe genotype likelihoods can be calculated from the read data alone.\n\nBut, if we combine that with allele frequencies in our model:\n\n![](diagrams/genos-and-reads-dag.png)\n\n...then we can also compute the posterior probability (by MCMC sampling)\nof each genotype.  If you are sampling from a single population, then this\ncan provide a better estimate of the true genotype.\n\n### Shiny Interlude #2\n\nFrom the `ngs-genotype-models` RStudio project that you downloaded previously:\n\n- Open `002-genotype-likelihoods-from-reads.Rmd`\n- Install any packages that RStudio tells you that you might need. (i.e. `install.packages(\"cowplot\")` if need be...)\n- Hit the \"Run Document\" Button\n\n(If this doesn't work for you, then you can access the Shiny app over the web:\n[https://eriqande.shinyapps.io/002-genotype-likelihoods-from-reads/](https://eriqande.shinyapps.io/002-genotype-likelihoods-from-reads/)\n)\n\nThen you can simulate genotypes, as before, but now, you can\nsimulate reads from those genotypes, and then compute genotype posteriors using\nMCMC.\n\n\n### Back to breakout rooms...some questions:\n\n1. What are the likelihoods for the three different possible genotypes when the read depth is 0 for an individual?\n2. What does it take for the likelihood to be highest for the heterozygote hypothesis?\n3. Is it more likely that a true heterozygote genotype will have a posterior probability that is highest for the hyothesis of \"common homozygote\" or the hypotheses of \"rare homozygote.\"\n4. How does the posterior distribution of the allele frequency computed from read data (the transparent blue histogram) compare to the posterior distribution if you know the genotypes exactly (the black line)?  How does this change when read depth is increased or decreased?\n5. If you only have a single read from a heterozygous individual, will that individual's maximum likelihood genotype ever be \"heterozygote.\"  What about its maximum _a-posteriori_ genotype? What are the conditions that lead to the heterozygous individual with only a single read having a high posterior probability of being a heterozygote?\n6. When read depths are low, even if you are calling genotypes using the highest posterior probability, do you expect the results to be very accurate?\n\nThere are more questions for thought at the bottom of the Shiny App Notebook, too.\n\n## One final hands on exercise\n\nYou might hear someone say, \"the great thing about using\ngenotype likelihoods, instead of called genotypes, with low-depth\nsequencing, is that you can _propagate the uncertainty downstream\nin your analysis._\"\n\nIn fact, I have said that sort of thing many times before (and probably\nhave said it several times today.)\n\nBut, what the hell does it really mean?\n\n### Shiny Interlude #3\n\nOur third exercise with a ShinyApp addresses this question, revealing\nsome of the unfortunate things that can happen if you call genotypes from low\ncoverage sequencing data and you treat them as known/certain.\n\nFrom the `ngs-genotype-models` RStudio project that you downloaded previously:\n\n- Open `003-read-inference-gsi.Rmd`\n- Install any packages that RStudio tells you that you might need. (i.e. `install.packages(\"cowplot\")` if need be...)\n- Hit the \"Run Document\" Button\n\n(If this doesn't work for you, then you can access the Shiny app over the web:\n[https://eriqande.shinyapps.io/003-read-inference-gsi/](https://eriqande.shinyapps.io/003-read-inference-gsi/)\n)\n\n\n\n## Wrap Up\n\nKey Take Home Messages:\n\n1. To the Bayesian, probability is how we measure uncertainty.\n2. The posterior is proportional to the prior times the likelihood.\n3. Monte Carlo sampling from a posterior distribution makes it very easy to assess the\nposterior distribution of any function of the samples.\n4. MCMC is useful to Bayesians because it can simulate from the posterior without needing\nto compute the normalizing constant.\n5. MCMC is great, but should be used with care.\n6. In their essence, models for calling genotypes probabilistically while estimating and\naccounting for allele frequencies are quite simple, and make a lot of sense.\n7. There are a lot of RAD/GBS-derived data sets of genotypes that show evidence\nof profoundly high rates of heterozygotes being miscalled as homozygotes. It doesn't seem\nout of the question that such error rates could influence downstream inferences.\n8. However, not all studies show very high genotyping error rates---high read depths and\nintelligent/stringent filtering can prevail.\n9. Probabilistic genotype calling is certainly a principled way to address these issues,\nbut it is not a panacea.  It still adheres to the principles of GIGO and can't make something\nfrom nothing.\n10. Don't fool yourself into thinking that by using probabilistic genotype calling\nyou won't lose anything when you shave sequencing effort down to as little as possible.\nIt is possible to\nlose a fair bit when you go down to low read depth / coverage.  But, it depends on what\nquestion you are trying to answer.\n\n\n\n\n\n",
    "supporting": [
      "probinf_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}