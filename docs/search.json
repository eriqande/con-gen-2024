[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Con-Gen-2024",
    "section": "",
    "text": "Overview/Welcome\nThis is a Quarto Book that holds most of the materials and links related to Eric C. Anderson’s presentations at ConGen 2024.\nThis year Eric is giving two different presentations with hands-on sessions:\nFor these two presentations, there is a narrative description that may be found in the pages of this book—each presentation constitutes a different chapter of this web-book. These chapters can be accessed from the contents links to the left of the page.\nThere are also slides and hands-on exercise that Eric will be using during the presentations. The slides were were made using Quarto’s revealjs format.\nThe links to the slides and the hands-on exercises are found below.\nNote, will you need to have R (and it is helpful to have RStudio) in order to do the hands-on exercises.",
    "crumbs": [
      "Overview/Welcome"
    ]
  },
  {
    "objectID": "index.html#probability-inference-and-genotype-likelihoods",
    "href": "index.html#probability-inference-and-genotype-likelihoods",
    "title": "Con-Gen-2024",
    "section": "Probability, Inference and Genotype Likelihoods",
    "text": "Probability, Inference and Genotype Likelihoods\n\nLECTURE SLIDES\nThe materials for the hands on does not run on the ConGen Rstudio Server. Rather, it must be run locally on your own computer using R and Rstudio. You should have R and Rstudio installed, with the package ‘tidyverse’ installed. With that setup you can get the RStudio project to run the hands on sessions with the following steps:\n\nif(!(\"usethis\" %in% rownames(installed.packages()))) {\n  install.packages(\"usethis\")\n}\nusethis::use_course(\"eriqande/ngs-genotype-models\")\n\nYou will need to answer the “Yes” response to a few questions. This will download an RStudio project and open it.\nFrom this RStudio project’s file browser, you can open the RMarkdown files, like: 001-allele-freq-estimation.Rmd.\nIf the message at the top of the file says you need some new packages, click the install option.\nThen Click the “Run Document” button.",
    "crumbs": [
      "Overview/Welcome"
    ]
  },
  {
    "objectID": "index.html#snakemake",
    "href": "index.html#snakemake",
    "title": "Con-Gen-2024",
    "section": "Snakemake",
    "text": "Snakemake\n\nLECTURE SLIDES\nTo do the hands on, you need to log into your RStudio Server account and use the Terminal to change directories into the Snakemake-Example directory that is in your home directory, i.e.:\n\ncd ~/Snakemake-Example\nIf you don’t have a Snakemake-Example directory in your home directory, then something has gone horribly wrong. If this is the case, try copying that folder from the instructor materials, using this:\ncd ~\ncp -r instructor_materials/Eric_Anderson/2024/Snakemake-Example ./",
    "crumbs": [
      "Overview/Welcome"
    ]
  },
  {
    "objectID": "index.html#parting-thoughts",
    "href": "index.html#parting-thoughts",
    "title": "Con-Gen-2024",
    "section": "Parting Thoughts",
    "text": "Parting Thoughts\n\nQuarto\nIf you haven’t heard about Quarto yet, it is RStudio’s successor to RMarkdown for making books, notebooks, and slide presentations. To learn more about Quarto books visit https://quarto.org/docs/books.\n\n\nBioinformatics Book\nFinally, as an aside. If you are starting to get into genomics and bioinformatics, you might be interested in checking out Eric’s Bioinformatics Handbook on the web. It is not complete, but it has some reasonably informative sections within it. (And it’s free!)\n\n\nOld Lectures Not Given This Year\n\nThe Coalescent\nIn 2023, Eric also gave an overview of the coalescent process and how it can be used to understand what genetic sequence data are expected to look like under different demographic scenarios. He isn’t doing it this year, but the slides are included here for the curious.\n\nLECTURE SLIDES\nLinks to R Notebooks for Hands-on",
    "crumbs": [
      "Overview/Welcome"
    ]
  },
  {
    "objectID": "probinf.html",
    "href": "probinf.html",
    "title": "1  Probability, inference, and genotype likelihoods",
    "section": "",
    "text": "1.1 Preamble\nIn this session, we will spend a little time talking about genotype likelihoods and “probabilistic genotype calling.”\nBut, I want to talk about these topics in the context of a little background on Bayesian statistics and Markov chain Monte Carlo.\nSo, my plan for this session is to start with a simple exercise of estimating an allele frequency in a Bayesian manner.\nFirst, we’ll talk about estimating the allele frequencies alone:\nAfter that we will put another layer into the model:\nFinally, we have one last Shiny App to play with that I hope will help people to understand what is meant by the phrase, “propagate uncertainty about genotypes into downstream inferences.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, inference, and genotype likelihoods</span>"
    ]
  },
  {
    "objectID": "probinf.html#preamble",
    "href": "probinf.html#preamble",
    "title": "1  Probability, inference, and genotype likelihoods",
    "section": "",
    "text": "Introduce acyclic directed graphs (DAGs)\nTalk about proportionality to the joint probability (a nice perspective)\nShow what inference of allele frequencies looks like if you know the genotypes exactly. (We have a Shiny App for that!)\nTalk about how we could learn about the posterior by simulating values from it. Whether they were correlated (MCMC) or not (vanilla Monte Carlo).\n\n\n\nAdd in the idea that genotypes are not observed directly, but rather underlie counts of reads of different alleles.\nShow that computing the posterior distribution in this case is much more complicated.\nProvide an intuitive explanation of how MCMC works in this case\nTalk a little bit about what MCMC is\nLet everyone explore genotype likelihoods and posteriors from simulated sequencing read data in another Shiny App.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, inference, and genotype likelihoods</span>"
    ]
  },
  {
    "objectID": "probinf.html#bayesian-inference",
    "href": "probinf.html#bayesian-inference",
    "title": "1  Probability, inference, and genotype likelihoods",
    "section": "1.2 Bayesian Inference",
    "text": "1.2 Bayesian Inference\n\n1.2.1 Personal Probability\nTo understand the Bayesian paradigm, it is useful to start with the awareness that everyone has their own probabilities for events or outcomes, and these probabilities are informed by each person’s experience, their prior beliefs, the data that they have access to, and the inferential perspectives or models that they can bring to the task of understanding and incorporating the information that they have available.\nWait! What kind of crazy notion of probability is this? Many of us might be familiar with the idea of probability being interpretable as the long-run average frequency of outcomes if a particular event is repeated many times. But this “personal probability” paradigm sounds quite different…\nA good way to approach this is with some simple card play. If I hand out cards from a shuffled deck to five people in the room and tell them not to look at the cards, and then ask everyone what the probability is that the next card on the deck is the ace of spades, most everyone will have the same probability for that. But if we let the 5 people look at their cards and then ask each, in turn, what they now think is the probability that the top card is the ace of spades, they will likely update their beliefs based on the card that they possess, as well as the responses of any cardholders before them.\nThere a lot of assumptions that go into these calculations. What are they?\nAt any rate, it is easy to see that people have different probabilities for the top card being an ace of spades depending on how much information they have.\n\n\n1.2.2 Probability as a “Measuring Stick” for Uncertainty\nA big switch being made here is that we are now using probability as a way of expressing or measuring each person’s “uncertainty” about an event. This is quite a departure from thinking about probability only as a property that can be discussed in connection with events that we can envision happening multiple time (so that we might derive a notion of probability as the average frequency of occurrence over very many trials.)\nThis interpretation of probability is considerably more flexible than the “frequentist” interpretation, and it is also already familiar to most of us: we are already quite comfortable with being told that the probability it is going to rain today is 30%, even though there will only ever be one today. With probability as a measure of uncertainty, you can use probabilities to talk about things that will only happen once. What is the probability that:\n\nThere will be three more category 5 hurricanes this season?\nNext year will be the hottest on record?\n\nNot only that, but now we can use probability to make statements about how uncertain we are about things that are deterministic. For example:\n\nWhat is your probability that Monrovia is the capital of Liberia?\n\nSo, if we are trying to measure:\n\nheight, then we use centimeters,\nmass, then we use kilograms,\nelectrical resistance, we use Ohms\nuncertainty?…we use probability…\n\n\n\n1.2.3 An interesting digression\nProbability as a field of mathematical study did not necessarily “grow up” with the intent to be a way to measure uncertainty. (It partly grew out of contemplation of “Borel subsets” and many much more esoteric concepts.) But the way that it was developed as a mathematical construct turns out to be precisely what is required if we desired to extend the system of logic in which 1 means “absolutely true” and 0 means “absolutely false” to a logical system in which there could be degrees of uncertainly between 0 and 1.\n\n\n1.2.4 Updating Beliefs\nA characteristic feature that we saw playing our card game is that when new information arrives, people update their beliefs, and hence they update their degree of undertainty (their probabilites…).\nClearly, if we want to be Bayesians and use probability to express uncertainty about things, we need a coherent, reasonable way to update our beliefs. This is done using basic principles from probability, which were described (for inferential purposes) by the Reverend Thomas Bayes (born 1702), and is hence known as Bayes’ Law, or Bayes Theorem.\nBayes Theorem is typically expressed in an equation that probably feels pretty opaque to most people (it certainly did to me when I was a young biology graduate student.) I remembered seeing it something like this: suppose that you have observed some data \\(D\\), and that you have \\(k\\) different hypotheses for its occurrence, and for each hypothesis, \\(i\\), you can compute \\(P(D|H_i)\\) the probability of observing D given that hypothesis \\(i\\) is true.\nThe posterior probability of a certain hypothesis \\(H_i\\) given \\(D\\) is then: \\[\nP(H_i|D) = \\frac{P(D|H_i)P(H_i)}{\\sum_{j = 1}^k P(D|H_j)P(H_j)}\n\\] Yack! Let’s see if we can find an easier way to talk about this!\nWe will (at least I hope it will be easier) and we will do so while talking about a simple problem that we can build upon to make more complex models: estimating the frequency of an allele in a population.\n\n\n1.2.5 A Simple Conditional Probability and DAGs\nAlmost all of the probabilities that we talk about routinely are conditional probabilities. That is, they are probabilities that are expressed conditional on some other variable or thing taking a certain value.\nHere we will start beating to death an example that involves genotype and allele frequencies, but bear with us, because we will end up using what we learn from this example—and building upon it—throughout this lecture.\nIf we ask about the probability that an individual is homozygous for the \\(A\\) allele, we might first have to make an assumption that the population is in Hardy-Weinberg equilibrium, and then we also need to assess this probability conditional on the frequency of the \\(A\\) allele being \\(p_A\\) in the population.\nSo, we could write: \\[\nP(\\mbox{Individual is AA}~|~p_A) = p_A^2\n\\] If we knew the value of \\(p_A\\), then it would be easy to do this calculation. This is an example of probability “running forward”. We will see later that when we want to do inference we need to “run probability backward” (which is what Bayes’ Theorem lets us do.)\nRecall that HWE means that the allelic type of the two gene copies carried in an individual are conditionally independent given the allele frequency. Hence, the probability of drawing the first gene copy as an A is \\(p_A\\), and the probability that the second gene copy is drawn as an A (given that the first was an \\(A\\)), is also \\(p_A\\). So the probability that the individual is \\(AA\\) is the product of those: \\(p_A^2\\).\nWe can draw a diagram of that called an acyclic directed graph (or DAG). These are very helpful for visualizing models as they become more complex.\n\nThe key thing to know about DAGs is that they express the factorization of the joint probability of the variables into a product of conditional probabilities. (Specifically a product over nodes of the probability of each node conditional on its parents.)\nFrom the way the DAG is drawn above, it is clear that we can say: \\[\nP(\\mbox{Individual is AA}~|~p_A) = P(Y_1 = A~|~p_A) P(Y_2 = A~|~p_A) = p_A \\cdot p_A =  p_A^2\n\\]\nHere is a quick summary of DAGs:\n\n\n\n1.2.6 A Picture of Inference of Allele Frequencies From a Sample of Genotypes\nThe previous discussion assumed that the allele frequency was known, and we wanted to compute the probability of an individual’s (unknown) genotype. For that exercise, we were running from an observed node in the graph (\\(p_A\\)) to two unobserved ones. This is a straightforward calculation of probabilities.\nMuch of the time in science, we are able to observe the variables at the end of the arrows in a DAG, and we want to make inferences about the variables that are “upstream” of them in a DAG. For an example, let us consider the situation in which we sample \\(N\\) diploids and observe their genotypes, and from that we want to infer the allele frequency \\(p_A\\).\nThe DAG for this situation looks like this:\n\nFrom which it is immediately apparent that the probability of all the data, say \\(\\boldsymbol{Y} = (Y_{1,1}, Y_{1,2}, Y_{2,1}, \\ldots, Y_{N,1}, Y_{N,2})\\) is just a simple product of Bernoulli random variables: \\[\nP(\\boldsymbol{Y}~|~p_A) = \\prod_{i=1}^N P(Y_{i,1}~|~p_A)P(Y_{i,2}~|~p_A)\n\\] In fact, if we define each \\(Y\\) to be 0 or 1 as follows: \\[\n\\begin{align}\nY = 0 & ~~~~~\\mbox{with probability} ~~~ 1 - p_A & ~~\\mbox{(i.e., it's an}~a) \\\\\nY = 1 & ~~~~~\\mbox{with probability} ~~~ p_A & ~~\\mbox{(i.e., it's an}~A)\n\\end{align}\n\\] Then it is not too hard to see that \\[\nP(\\boldsymbol{Y}~|~p_A) = p_A^{\\sum Y_{i,j}} (1- p_A)^{2N - \\sum Y_{i,j}}\n\\] This is a probability function. But if you consider this as a function of \\(p_A\\) with \\(\\boldsymbol{Y}\\) considered as fixed, then it is often referred to as the likelihood function.\n\n\n1.2.7 Methods of Inference\nWe see that we are trying to learn something (make inference) about \\(p_A\\) (an unshaded/unobserved variable) that is upstream of our observed data. This is inference.\nTo put some concrete numbers on this. Let’s say that \\(N = 100\\) diploids, and out of the 200 gene copies, 73 of them were \\(A\\) alleles.\nThere are lots of ways that you might do inference. Here are a few:\n\nMethod of the Eyeball: Look at your sample and surmise that the fraction of A alleles in the sample is a good estimate of the fraction of \\(A\\) alleles in the population.\nMethod of Moments: This formalizes the “Method of the Eyeball” by equating the sample mean with the population mean.\nMethod of Maximum Likelihood: Find the value of \\(p_A\\) that maximizes the probability of observing your sample. In other words, consider the probability \\(P(\\boldsymbol{Y}~|~p_A)\\) as a function of \\(p_A\\), where the data, \\(\\boldsymbol{Y}\\), are fixed, and then find the value of \\(p_A\\) that maximises that likelihood function.\n\nAll of those methods give you a point estimate for \\(p_A\\). An alternative to these methods is to be Bayesian and find the posterior distribution for \\(p_A\\) conditional on the data, \\(\\boldsymbol{Y}\\).\nBefore we do this, we are going to want to review a view simple facts about marginal, conditional, and joint probabilities.\n\n\n1.2.8 Some important probability rules\nIf you have two different events \\(A\\) (no relation, necessarily, to the big-A allele ) and \\(B\\), and we use \\(A\\) and \\(B\\) to refer to the outcome of each, then here are some things that are always true:\n\n\\(P(A)\\) and \\(P(B)\\) are referred to as marginal probabilities.\nThe joint probability of \\(A\\) and \\(B\\) is the probability that those two outcomes occurred, and it can be computed as the product of a marginal probability and a conditional probability, in two different ways: \\[\nP(A,B) = P(A)P(B~|~A) = P(B)P(A~|~B)\n\\]\nThis means that conditional probabilities can be computed from the joint probability: \\[\nP(A~|~B) = \\frac{P(A,B)}{P(B)}~~~~~~~~~~~~~~~~~~~~~~~~\nP(B~|~A) = \\frac{P(A,B)}{P(A)}\n\\]\n\nAnd that leads us to an expression for Bayes Theorem that I find easier to grok out: \\[\nP(A~|~B) \\propto P(A, B)\n\\] where we are thinking of \\(P(A|B)\\) as a function of \\(A\\) with \\(B\\) fixed. It is typically easy to compute the joint probability, \\(P(A,B)\\), and then you just have to remember that \\(P(A~|~B)\\), since it is a probability on \\(A\\), must sum to one over all possible values of \\(A\\).\nThe same is true for: \\[\nP(B~|~A) \\propto P(A, B)\n\\]\n\n\n1.2.9 Bayesian Inference for \\(p_A\\)\nBack to our simple example. If we want to do Bayesian inference for \\(p_A\\) we see that we will want to compute the posterior probability: \\[\nP(p_A|\\boldsymbol{Y})\n\\] which we now know is going to be proportional to the joint probability: \\[\nP(p_A,\\boldsymbol{Y})\n\\] and what we currently have at our disposal is the likelihood: \\[\nP(\\boldsymbol{Y}~|~p_A)\n\\] We could get the joint probability by using the likelihood in the product: \\[\nP(p_A,\\boldsymbol{Y}) = P(\\boldsymbol{Y}~|~p_A) P(p_A)\n\\] But what is this \\(P(p_A)\\)?!\nIt is the prior distribution for \\(p_A\\). It is a necessary ingredient to be able to use the likelihood to compute the joint probability (and, hence, the posterior probability), and we envision it as a probability distribution that expresses our degree of belief about \\(p_A\\) before we look at the data.\nNote that this all boils down verbally to: The posterior distribution is proportional to the prior times the likelihood.\n\n\n1.2.10 A family of priors for \\(p_A\\)\nSince \\(p_A\\) is a proportion, an obvious choice for prior would be a beta distribution. The beta distribution gives a continuous prior distribution on a value that is between 0 and 1. It has two parameters, often called \\(\\alpha_1\\) and \\(\\alpha_2\\). Here are some examples:\n\n\n\n\n\n\n\n\n\nThe beta density for a random variable \\(X\\) has the form: \\[\np(x | \\alpha_1, \\alpha_2) = \\frac{\\Gamma(\\alpha_1 + \\alpha_2)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)}\nx^{\\alpha_1 - 1}(1-x)^{\\alpha_2 - 1}\n\\] The part that looks hairy is a few Gamma functions. Don’t worry about those—it is a constant. The important part (the “kernel”, as they say…) is: \\[\nx^{\\alpha_1 - 1}(1-x)^{\\alpha_2 - 1}\n\\] Or, if we wanted this to a be a prior on \\(p_A\\), the prior would be proportional to: \\[\n{p_A}^{\\alpha_1 - 1}(1-p_A)^{\\alpha_2 - 1}\n\\] And, if we wanted to be even more specific, we could choose \\(\\alpha_1 = \\alpha_2 = 1\\) to give ourselves a uniform prior which is proportional to 1: \\[\nP(p_A) \\propto {p_A}^{1 - 1}(1-p_A)^{1 - 1} = {p_A}^{0}(1-p_A)^{0} = 1\n\\]\n\n\n1.2.11 A Graphical Aside\nJust wanting to point out that if you want to be a Bayesian, you can’t have any unobserved nodes on your DAG that don’t have any parents. Those priors that you use/accept/assume are always the uppermost nodes in a DAG. I usually use gray nodes to denote them. For example, here is the DAG for the Bayesian version of the previously shown DAG:\n\n\n\n1.2.12 The posterior for \\(p_A\\)\nNow that we have a choice for our prior, \\(P(p_A)\\), we can easily find something that is proportional to the posterior distribution. What is proportional to the posterior distribution? Remember, The posterior distribution is proportional to the prior times the likelihood (which, don’t forget, is the joint probability). Remember that the likelihood looks like: \\[\nP(\\boldsymbol{Y}~|~p_A) = p_A^{\\sum Y_{i,j}} (1- p_A)^{2N - \\sum Y_{i,j}}\n\\] And, in our example, the number of \\(A\\) alleles is 73 (out of 200), so \\(\\sum Y_{i,j} = 73\\) and \\(2N - \\sum Y_{i,j} =  127\\).\nSince we are only going to worry about finding something that the posterior probability is proportional to, we can drop any constants and we get this for the prior times the likelihood: \\[\n\\begin{align}\nP(p_A~|~\\boldsymbol{Y}) &\\propto {p_A}^{0}(1-p_A)^{0} \\times p_A^{73}(1-p_A)^{127} \\\\\n  & \\propto p_A^{73}(1-p_A)^{127} \\\\\n  & \\propto p_A^{74 - 1}(1-p_A)^{128 - 1} \\\\\n\\end{align}\n\\] which is precisely the kernel of a beta distribution with parameters 74 and 128.\nAha! So, the posterior here is a beta distribution with \\(\\alpha_1 = 74\\) and \\(\\alpha_2 = 128\\). We can plot that if we want:\n\n\n\n\n\n\n\n\n\n\n\n1.2.13 Shiny Interlude #1\nIn fact, we can do one better than just plotting that. I’ve prepared a Shiny Notebook that lets you play with different values of the allele frequency (\\(p_T\\) in this case—the frequency of the \\(T\\) base at a genome position), simulate diploid genotype data from that, and then compute the posterior distribution, whilst fiddling with the prior parameters, if desired.\nThe one downside of Shiny apps is that they don’t run properly on the RStudio Server that has been set up for ConGen. Nor will they run properly in the Remote Desktop version of the same.\nBut this is not a huge issue, since you should be able to run it on your own computer with RStudio. The procedure for that is\n\nOpen up RStudio.\nIn the R console paste this code:\n\nif(!(\"usethis\" %in% rownames(installed.packages()))) {\n  install.packages(\"usethis\")\n}\nusethis::use_course(\"eriqande/ngs-genotype-models\")\n\nYou will need to answer the “Yes” response to a few questions. This will download an RStudio project and open it.\nFrom this RStudio project’s file browswer, open the file 001-allele-freq-estimation.Rmd.\nIf the message at the top of the file says you need some new packages, click the install option.\nThen Click the “Run Document” button.\n\nIf that does not work for some of the students: you can go directly to the website: https://eriqande.shinyapps.io/001-allele-freq-estimation/. But, be warned that if too many people use that link it will overwhelm my free ShinyApps account.\nNow, I am going to break students into break-out groups of 4 or 5 (randomly) for about 15 minutes.\nTake a few minutes to introduce yourselves to one another, then play with the Shiny app and talk to one another about it as you do. Maybe even work togethery to do these things:\n\nInput beta distribution parameters to get these different shapes:\n\nan upward facing U\na flat line\na curve that keeps going up at one of the boundaries\na hill with a peak at 0.3\n\nObserve how the posterior distribution changes when sample size changes.\nWith a true allele frequency of \\(p_T = 0\\), with a sample size of 50 diploids, find values of the prior parameters that will give you a posterior centered on 0.5 (in other words a ridiculously heavy prior…)\n\nI’ll try to say hello in the different break out rooms.\n\n\n1.2.14 To the Bayesian, the posterior distribution is everything\nTo the Bayesian, the posterior distribution contains all the information that the data has about the parameter to be estimated—in this case, \\(p_A\\). And, the full posterior distribution, unsummarized, expresses that all that information as fully as can be done.\nHowever, sometimes you don’t want to carry around a full posterior distribution, especially if it is a distribution on multiple variables (multidimensional).\nOften you will want to summarise the posterior distribution.\n\n\n1.2.15 Some standard ways of summarizing the posterior\n\nThe posterior mean. Gives a point estimate that minimizes squared error posterior loss.\nThe posterior median. Gives a point estimate that minimizes absolute posterior loss.\nThe posterior mode. Gives a point estimate that minimizes 0-1 posterior loss.\nThe Bayesian Credible Interval: provides an interval estimator. Like a confidence interval, but infinitely easier to interpret correctly…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, inference, and genotype likelihoods</span>"
    ]
  },
  {
    "objectID": "probinf.html#monte-carlo-sampling-from-a-posterior-distribution",
    "href": "probinf.html#monte-carlo-sampling-from-a-posterior-distribution",
    "title": "1  Probability, inference, and genotype likelihoods",
    "section": "1.3 Monte Carlo sampling from a posterior distribution",
    "text": "1.3 Monte Carlo sampling from a posterior distribution\nIn the above segment, we obtained a posterior distribution analytically. That is not the way things typically work out. It is more common that one is not able to obtain the posterior analytically, but, rather, can only obtain a sample from the posterior distribution. With a sample from the posterior, however, it is possible to use Monte Carlo to approximate virtually any quantity that might be of interest. Let’s look at that with our simple example.\n\n1.3.1 A “vanilla” Monte Carlo sample from the posterior for \\(p_A\\)\nEven though we can compute the posterior analytically, it can still be convenient to obtain a sample from it. We can sample 1 million values from the posterior for \\(p_A\\) like this:\n\npA &lt;- rbeta(n = 10^6, 74, 128)\n\nThe first 10 values of that sample look like:\n\npA[1:10]\n\n [1] 0.3037068 0.3696474 0.3656824 0.3726274 0.4189819 0.3861043 0.4017063\n [8] 0.3975998 0.3898352 0.4057108\n\n\nWe call this a “vanilla” Monte Carlo sample because every member of the sample was independent—this is not Markov chain Monte Carlo.\nIf we wanted to use the sample to approximate the full posterior, we could do that with a histogram:\n\nhist(pA, breaks = 100)\n\n\n\n\n\n\n\n\nIf we want the posterior mean, that is easy, too:\n\nmean(pA)\n\n[1] 0.366348\n\n\nas is the posterior median:\n\nmedian(pA)\n\n[1] 0.3658844\n\n\nOr the standard deviation of the posterior distribution:\n\nsd(pA)\n\n[1] 0.03382894\n\n\nOr, the 90%-equal-tail Credible Interval\n\nquantile(pA, probs = c(0.05, 0.95))\n\n       5%       95% \n0.3114582 0.4226707 \n\n\nAll of those quantities could have been obtained analytically in this case, but it is a lot simpler to just work with a Monte Carlo sample because the operations are the same with every Monte Carlo sample (which is not necessarily true of every analytical distribution…).\n\n\n1.3.2 Transformations are simple with a Monte Carlo sample\nImagine for a moment that what you really wanted to estimate was the probability that, if you sampled three individuals, in sequence, from the population, the first one would be homozygous \\(AA\\), the second one would be heterozygous \\(Aa\\) and the third one would be homozygous \\(aa\\). Since we are being Bayesians here, we will want to compute the posterior distribution of that probability given the observed data. This is something that would be quite messy to do analytically, but it is easy to use the Monte Carlo sample to do it. Clearly, for a given \\(p_A\\), the probability of the sequence (\\(AA\\), \\(Aa\\), \\(aa\\)) is \\(p_A^2\\times 2 p_A (1-p_A) \\times (1-p)^2\\). So, the posterior distribution of that quantity is simply:\n\nthree_geno_seq_posterior &lt;- pA^2 * 2 * pA * (1 - pA) * (1 - pA)^2\n\nAnd we can look at it:\n\nhist(three_geno_seq_posterior, breaks = 100)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, inference, and genotype likelihoods</span>"
    ]
  },
  {
    "objectID": "probinf.html#probcall",
    "href": "probinf.html#probcall",
    "title": "1  Probability, inference, and genotype likelihoods",
    "section": "1.4 Probabilistic Genotype Calling / Allele frequency estimation",
    "text": "1.4 Probabilistic Genotype Calling / Allele frequency estimation\nWe are now going to expand our allele frequency model just a little bit, to get a taste for how we might infer both allele frequencies and genotypes given short read data.\nLet’s consider this setup: we are trying to infer genotypes in individuals and estimate the allele frequencies in a population at a SNP in which the variants are the two different bases, \\(C\\) or \\(T\\). We know where the SNP occurs (we are disregarding the problem of inferring whether a SNP is there) but we do not know its frequency in the population. We have sequenced \\(N\\) individuals at this SNP. We don’t know the genotype of any of these individuals, but we get to observe the number of sequencing reads on each individual, \\(i\\), that contain a \\(C\\), and the number of reads that contain a \\(T\\) at the site. We call that observed variable \\(\\boldsymbol{R}_i = (R_{i,C}, R_{i,T})\\), a two-vector. We include in our model a sequencing error rate \\(\\mu\\) which, for simplicity here, we will assume is known. With probability \\(\\mu\\) a read from a chromosome that really contains a \\(C\\) at the site will report a \\(T\\), while with probability \\(1 - \\mu\\) the base at the site is correctly reported as a \\(C\\). Similarly for chromosomes that contain a \\(T\\) at the site, with probability \\(\\mu\\) a read will report a \\(C\\) and with probability \\(1 - \\mu\\) it will correctly report a \\(T\\).\nHere is a DAG for the model:\n\nAll right. This is fun. Now there are two layers of unknown variables (the \\(Y\\)’s and \\(p_C\\) that we will try to infer together, at the same time).\n\n1.4.1 Why is this a good idea?\nBecause the allele frequency estimate is there to inform the inference of genotypes.\nIf the frequency of the \\(C\\) allele is quite low, that will provide a lot of evidence in the model against inferring someone to be homozygous for the \\(C\\) allele.\nAnd this would be a good thing, because after a little bit of looking at published data sets of genotypes called by traditional means (i.e. apparently without considering allele frequencies), it appears there is a bit of an epidemic of genotyping error in RAD sequenced data.\n\n\n1.4.2 Conditional probabilities in the model\nMost of these we have seen before:\n\n\\(P(p_C | \\alpha_C, \\alpha_T)\\) is just a beta prior on \\(p_C\\).\n\\(Y_{i,1}\\) is a Bernoulli trial with success probability (i.e. probability of getting a \\(C\\) of \\(p_C\\).)\nThe only new one is the probability distribution of read counts given the genotype and the genotyping error rate: \\(P(\\boldsymbol{R}_i~|~Y_{i,1}, Y_{i,2}, \\mu)\\)\n\n\n\n1.4.3 The conditional probability \\(P(\\boldsymbol{R}_i~|~Y_{i,1}, Y_{i,2}, \\mu)\\)\nThis thing turns out to be pretty easy. It can be broken down into cases:\n\n\\(i\\) is homozygous for \\(C\\) (i.e., \\(Y_{i,1} = Y_{i,2} = C\\)): In this case, the only way to get a read with a \\(T\\) at the site is by a sequencing error, which happens with probability \\(\\mu\\). Thus, it is akin to drawing \\(M = R_{i,C} + R_{i,T}\\) balls out of an urn (with replacement), a fraction \\(\\mu\\) of which say \\(T\\) on them, the rest which say \\(C\\) on them. In such a case \\(R_{i,C}\\) is binomially distributed, \\(M\\) draws with success probability \\(1-\\mu\\).\n\\(i\\) is homozygous for \\(T\\) (i.e., \\(Y_{i,1} = Y_{i,2} = T\\)): Same argument as above, \\(R_{i,C}\\) is binomially distributed, \\(M\\) draws with success probability \\(\\mu\\) this time, rather than \\(\\mu\\).\n\\(i\\) is heterozygous (i.e., \\((Y_{i,1} = C, Y_{i,2} = T)\\) or \\((Y_{i,1} = T, Y_{i,2} = C)\\)): Because we have assumed sequencing error rates are symmetrical between different bases, this is easily figured out—the reads are like balls you are sampling (with replacement) from an Urn in which half of the balls are \\(T\\)’s and half of them are \\(C\\)’s.\n\nNote that when this probability is considered a function of the genotype, (with the observed reads treated as fixed), it is know as the genotype likelihood: \\[\nL(Y_{i,1}, Y_{i,2}; \\boldsymbol{R}_i, \\mu) = P(\\boldsymbol{R}_i~|~Y_{i,1}, Y_{i,2}, \\mu)\n\\]\nAn easier way to see this might be a picture:\n\nGenotype likelihoods are measures of how much support the read data gives to the three possible genotypes (\\(CC\\), \\(CT\\), or \\(TT\\), in this case).\nThey are a critical ingredient for using read information for doing population genomics, but they are only part of the story.\n\n\n1.4.4 Some of the assumptions in the above model…\nThere are quite a few but the biggest is that each read is independent of the others. That assumption would be violated if:\n\nthe sequences upon which the \\(C\\) (or the \\(T\\)) occurred were somehow subject to greater amplification during any PCR steps, or were more likely to attach to “the lawn” on an Illumina machine.\nthere are PCR duplicates. This can be a really big issue for RAD methods that don’t have a good mechanism for filtering PCR duplicates.\n\n\n\n1.4.5 An intuitive picture of how Monte Carlo might proceed\nThe unobserved genotypes of the individuals in the above model are what are sometimes called “missing data”. They are not missing data in the sense of small holes in your data set that resulted from some data-collection failure. They are missing in the sense of “What is missing here is something that would make this whole problem easier”. Clearly, as we showed in an earlier section, if we knew these genotypes exactly, doing Bayesian inference for \\(p_C\\) is pretty darn easy.\nAnother name for this “missing data” is “latent variables.” Many formulations of MCMC are developed with these types of latent variables. In our case, a brief sketch of how MCMC would proceed starts with a bit of wishful thinking. We say, “Wouldn’t it be great if we actually knew the value of \\(p_C\\)…Hey! We don’t, but let’s just start with a guess and pretend it is true.” So, we would start by initializing the model and setting \\(p_C\\) to some random value, let us call that value \\(p_C^{(0)}\\).\nNow, if we know the value of \\(p_C\\), then the only thing that would be left unknown in the model would be the genotypes of the individuals. But looking at the graph it is clear that they are all conditionally indendent given \\(p_C^{(0)}\\) and the observed read depths. So, pretending that \\(p_C^{(0)}\\) is the truth, we could compute, for each individual \\(i\\), the posterior probabilities of \\(i\\)’s genotype (the three cases, 1–3 above). (By the way, this distribution is often called the “full conditional distribution”).\nThen we would simulate values of the genotypes from those posterior distributions. Aha! once we have those simulated, we can pretend that they are real and use them to come up with a new estimate of \\(p_C\\) by computing the posterior distribution (full conditional) for \\(p_C\\) given the current values of the genotypes and then simulating a value from that distribution. Call that \\(p_C^{(1)}\\), and then go back to simulating new values for the genotypes.\nDoing that over and over again creates a Markov chain that does a random walk over the space of all possible genotypes of the \\(i\\) individuals and the unknown allele frequency \\(p_C\\), with the amount of time the chain spends in any state being proportional to the posterior probability of that state!.\nThis is actually Markov chain Monte Carlo.\n\n\n1.4.6 What is the Markov Chain part of MCMC?\nWe’ve seen that Monte Carlo in this Bayesian context is all about basing one’s inference on a sample from the posterior distribution. However, it turns out that you often can’t obtain independent samples from a complex posterior distribution. In high dimensions that is just not possible. But, it is possible to construct an ergodic Markov chain that has as its limiting distribution the posterior distribution you want to be sampling from. In fact, that is what our “wishful thinking” procedure with the “missing data”, described above, is actually doing.\nWhat?!\nOK, we will talk briefly about Markov chains. Basically a Markov chain is a stochastic process (like a random walk) in which the probability of ending up in some place after the next step depends only on where it currently is—not on how it got there…\nA classic example is a random walk, we will consider one with scattering boundaries:\n\nThe important take-home message from the tpm Computer Demo is that if you run a Markov chain long enough, it spends time in different states with a predictable pattern called the limiting distribution. In other words, you can take a sample of the states visited by the chain and use that as a Monte Carlo sample. That is Markov chain Monte Carlo.\n\n\n1.4.7 Why MCMC is so great for Bayesians\nVia MCMC, it is possible to obtain samples from the posterior distribution even if the posterior distribution is highly-multidimensional and complicated.\nThere are two main reasons for this:\n\nA Markov chain can be devised to sample from the posterior distribution without knowing the normalizing constant. In other words, if you can compute the joint probability (prior times likelihood) for a fully-observed version of your model, that is all you need. (Note that you can pretty much always compute the joint probability for a fully-observed model.)\nThe Markov chain so devised can be constructed by proposing changes only to small parts of the model, which makes it possible to break down complex distributions in ways that make it manageable to sample from them.\n\nSadly, the details of this are beyond the scope of a 40 minute lecture and a few hands-on sessions.\n\n\n1.4.8 Why MCMC can be not-so-great\n\nIt can be computationally expensive and take a long time.\nIt can mix poorly, i.e. it might not sample reliably from all areas of the distribution in proportion to (or even approximately in proportion to) their posterior probability.\nIt can appear to be mixing well, but actually not be mixing well. It is important to perform multiple runs from different starting values to assess convergence.\nEspecially in “canned packages” it can let you do “black-boxy” inference in models that are so complicated that it is hard to have proper intuition about how they behave or perform given the vararies of your own data.\n\nAgain, these topics are beyond the scope of this presentation. But grab me at some point if you want to talk about it, or consider the MCMC course at the Summer Institute in Statistical Genetics.\n\n\n1.4.9 Playing with genotype likelihoods and genotype posteriors\nThe genotype likelihoods can be calculated from the read data alone.\nBut, if we combine that with allele frequencies in our model:\n\n…then we can also compute the posterior probability (by MCMC sampling) of each genotype. If you are sampling from a single population, then this can provide a better estimate of the true genotype.\n\n\n1.4.10 Shiny Interlude #2\nFrom the ngs-genotype-models RStudio project that you downloaded previously:\n\nOpen 002-genotype-likelihoods-from-reads.Rmd\nInstall any packages that RStudio tells you that you might need. (i.e. install.packages(\"cowplot\") if need be…)\nHit the “Run Document” Button\n\n(If this doesn’t work for you, then you can access the Shiny app over the web: https://eriqande.shinyapps.io/002-genotype-likelihoods-from-reads/ )\nThen you can simulate genotypes, as before, but now, you can simulate reads from those genotypes, and then compute genotype posteriors using MCMC.\n\n\n1.4.11 Back to breakout rooms…some questions:\n\nWhat are the likelihoods for the three different possible genotypes when the read depth is 0 for an individual?\nWhat does it take for the likelihood to be highest for the heterozygote hypothesis?\nIs it more likely that a true heterozygote genotype will have a posterior probability that is highest for the hyothesis of “common homozygote” or the hypotheses of “rare homozygote.”\nHow does the posterior distribution of the allele frequency computed from read data (the transparent blue histogram) compare to the posterior distribution if you know the genotypes exactly (the black line)? How does this change when read depth is increased or decreased?\nIf you only have a single read from a heterozygous individual, will that individual’s maximum likelihood genotype ever be “heterozygote.” What about its maximum a-posteriori genotype? What are the conditions that lead to the heterozygous individual with only a single read having a high posterior probability of being a heterozygote?\nWhen read depths are low, even if you are calling genotypes using the highest posterior probability, do you expect the results to be very accurate?\n\nThere are more questions for thought at the bottom of the Shiny App Notebook, too.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, inference, and genotype likelihoods</span>"
    ]
  },
  {
    "objectID": "probinf.html#one-final-hands-on-exercise",
    "href": "probinf.html#one-final-hands-on-exercise",
    "title": "1  Probability, inference, and genotype likelihoods",
    "section": "1.5 One final hands on exercise",
    "text": "1.5 One final hands on exercise\nYou might hear someone say, “the great thing about using genotype likelihoods, instead of called genotypes, with low-depth sequencing, is that you can propagate the uncertainty downstream in your analysis.”\nIn fact, I have said that sort of thing many times before (and probably have said it several times today.)\nBut, what the hell does it really mean?\n\n1.5.1 Shiny Interlude #3\nOur third exercise with a ShinyApp addresses this question, revealing some of the unfortunate things that can happen if you call genotypes from low coverage sequencing data and you treat them as known/certain.\nFrom the ngs-genotype-models RStudio project that you downloaded previously:\n\nOpen 003-read-inference-gsi.Rmd\nInstall any packages that RStudio tells you that you might need. (i.e. install.packages(\"cowplot\") if need be…)\nHit the “Run Document” Button\n\n(If this doesn’t work for you, then you can access the Shiny app over the web: https://eriqande.shinyapps.io/003-read-inference-gsi/ )",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, inference, and genotype likelihoods</span>"
    ]
  },
  {
    "objectID": "probinf.html#wrap-up",
    "href": "probinf.html#wrap-up",
    "title": "1  Probability, inference, and genotype likelihoods",
    "section": "1.6 Wrap Up",
    "text": "1.6 Wrap Up\nKey Take Home Messages:\n\nTo the Bayesian, probability is how we measure uncertainty.\nThe posterior is proportional to the prior times the likelihood.\nMonte Carlo sampling from a posterior distribution makes it very easy to assess the posterior distribution of any function of the samples.\nMCMC is useful to Bayesians because it can simulate from the posterior without needing to compute the normalizing constant.\nMCMC is great, but should be used with care.\nIn their essence, models for calling genotypes probabilistically while estimating and accounting for allele frequencies are quite simple, and make a lot of sense.\nThere are a lot of RAD/GBS-derived data sets of genotypes that show evidence of profoundly high rates of heterozygotes being miscalled as homozygotes. It doesn’t seem out of the question that such error rates could influence downstream inferences.\nHowever, not all studies show very high genotyping error rates—high read depths and intelligent/stringent filtering can prevail.\nProbabilistic genotype calling is certainly a principled way to address these issues, but it is not a panacea. It still adheres to the principles of GIGO and can’t make something from nothing.\nDon’t fool yourself into thinking that by using probabilistic genotype calling you won’t lose anything when you shave sequencing effort down to as little as possible. It is possible to lose a fair bit when you go down to low read depth / coverage. But, it depends on what question you are trying to answer.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability, inference, and genotype likelihoods</span>"
    ]
  },
  {
    "objectID": "snakemake.html",
    "href": "snakemake.html",
    "title": "2  A brief introduction to Snakemake",
    "section": "",
    "text": "2.1 Narrative Explanation of Snakemake\nFor managing these sorts of bioinformatic workflows there is a Python-based framework called Snakemake that is well-supported, flexible, and incredibly powerful. Understanding how Snakemake works and becoming familiar with its many features involves a non-trivial learning curve; however, for anyone spending a sizable chunk of their graduate career managing bioinformatic analyses, or for anyone that is a lab bioinformatician running bioinformatic workflows across many species (for example), the benefits of learning Snakemake will continue to pay dividends for years to come.\nSomeone who has mastered all the topics in Part I of this handbook (Unix programming, working on remote computers, etc.) certainly will have the skills to write what I will call input-oriented, forward-marching workflows. I call these “forward-marching” because they start from a set of input files (for example files of sequences from a sequencer), and then the workflow is defined as a series of sequential steps, one after the other. They are “input-oriented” in the sense that such a workflow starts with a series of inputs, but the workflow itself doesn’t really know what it is trying to produce from those inputs until it has run all the way through and actually turned those inputs into outputs. For example, all the input files might get trimmed or cleaned, then they would all get mapped to a genome, and then those mapped sequences would be used to call variants, so as to find SNPs in the data, etc. If you were writing this in an input-oriented, forward-marching fashion, then, to deal with the fact that you had multiple files of DNA sequences (for example, one for each individual bird or fish that had been sampled), you might write Unix for loops to cycle over all the input files as in Section @ref(unix-for-loops), or, you could define a SLURM job array to start a separate job instance for each input file, as in Section @ref(slurm-job-arrays). In each case, you would have to do some extra programming to deal with the different input files, and if one of the steps of your workflow failed on just one, or a few, of the files, you might spend a large amount of time tracking those failures down and than manually re-running that small number of jobs.\nBy contrast, Snakemake takes a different approach to managing workflows. We will call it an output-oriented, backward-looking approach. We call it that because workflows in Snakemake are defined first and foremost in terms of the output files that are desired, along with instructions on how to create those output files from necessary input files and bioinformatic programs. They are backward-looking in the sense that, once you have developed a Snakemake workflow, you get results by telling Snakemake which output files you want to create, and then it “looks backwards” through the workflow to determine which input files are needed to create the requested outputs. Sometimes it has to look backwards through several steps before it identifies all the necessary input files. Once it has found those necessary inputs, it then runs forward through the steps to create the output files. In this phase, the workflow looks like it is “forward-marching”, in the sense that outputs are being created from inputs. But, in order to get to that “forward-running” phase, Snakemake had to look backward to figure out what inputs to use.\nThe above constitutes some subtle points, that might not be clear upon first reading, but we will try to summarize it in a few pithy phrases:\nThere are many advantages to this output-oriented, backward-looking approach:\nIn addition to these obvious advantages of the output-oriented approach, Snakemake also includes a number of features that make it easy to use your workflow on a variety of different platforms. It is tightly integrated with conda (Section @ref(miniconda)), letting the user define conda environments for each step in the workflow. This means that if you move your whole workflow to a new cluster, you don’t have to spend any time coordinating the installation of the programs you need—if you set things up properly with Snakemake and conda, that will happen automatically. If you distribute your workflows to other people to use, this is particularly helpful, since you will spend far less time assisting them in setting up their computer environment to run your workflow. Snakemake can also be customized to work in your own cluster environment. Finally, there are interfaces to allow your Snakemake workflows to run seamlessly in the cloud.\nFull documentation for Snakemake can be found at https://snakemake.readthedocs.io/en/stable/. This documentation is comprehensive, but can feel a little daunting at first. On the other hand, the developers of Snakemake also provide an excellent and accessible tutorial at https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A brief introduction to Snakemake</span>"
    ]
  },
  {
    "objectID": "snakemake.html#narrative-explanation-of-snakemake",
    "href": "snakemake.html#narrative-explanation-of-snakemake",
    "title": "2  A brief introduction to Snakemake",
    "section": "",
    "text": "A workflow defined as a typical Unix shell script can be thought of as a process that runs forward. You give it a lot of input files and it just cranks through a bunch of steps until the output files are made.\nA workflow defined with Snakemake works differently. First you define the workflow in terms of a series of “rules.” Then, when you request any given set of output files, Snakemake will look backwards through the rules of the workflow and figure out exactly which steps must be performed, on which input files, in order to create the requested output files. Once it has determined that, it runs just those necessary steps.\n\n\n\nIf your workflow has many steps, and some of them have already been run, then Snakemake automatically recognizes that, and will not re-run steps in the workflow that have already been completed. In an input-oriented system (like a traditional Unix script), you would have to spend the time to figure out which steps had already been run, and then run your script only from that point forward. Doing so can be a hassle and can also be prone to errors.\nA workflow defined by Snakemake, being explicit about the inputs needed for each step, naturally defines “work units” that can be run independently of one another. Accordingly, Snakemake, itself, can break a huge bioinformatic workflow into a number of small jobs that can be run in parallel, saving you, the user, from having to write scripts to launch a series of SLURM job arrays.\nThe fact that Snakemake automatically keeps track of which inputs already exist—and which might still need to be generated—provides huge benefits when some of your jobs fail. Anyone who has used a cluster has stories about jobs that inexplicably fail. Without a workflow management system like Snakemake, you can spend almost as much of your own time managing these failed jobs as it took to launch all the jobs in the first place.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A brief introduction to Snakemake</span>"
    ]
  }
]